/projectnb/manoslab/cgokmen/venvs/rsvd_compress/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/projectnb/manoslab/cgokmen/venvs/rsvd_compress/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/projectnb/manoslab/cgokmen/venvs/rsvd_compress/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/projectnb/manoslab/cgokmen/venvs/rsvd_compress/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
evaluating original...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.54s/it]
  0%|          | 0/166 [00:00<?, ?it/s]  1%|          | 1/166 [00:02<07:56,  2.89s/it]  1%|          | 2/166 [00:04<06:14,  2.28s/it]  2%|▏         | 3/166 [00:06<05:41,  2.10s/it]  2%|▏         | 4/166 [00:08<05:25,  2.01s/it]  3%|▎         | 5/166 [00:10<05:16,  1.96s/it]  4%|▎         | 6/166 [00:12<05:09,  1.93s/it]  4%|▍         | 7/166 [00:14<05:03,  1.91s/it]  5%|▍         | 8/166 [00:15<04:59,  1.90s/it]  5%|▌         | 9/166 [00:17<04:56,  1.89s/it]  6%|▌         | 10/166 [00:19<04:53,  1.88s/it]  7%|▋         | 11/166 [00:21<04:51,  1.88s/it]  7%|▋         | 12/166 [00:23<04:49,  1.88s/it]  8%|▊         | 13/166 [00:25<04:47,  1.88s/it]  8%|▊         | 14/166 [00:27<04:45,  1.88s/it]  9%|▉         | 15/166 [00:29<04:43,  1.88s/it] 10%|▉         | 16/166 [00:30<04:41,  1.88s/it] 10%|█         | 17/166 [00:32<04:39,  1.88s/it] 11%|█         | 18/166 [00:34<04:37,  1.88s/it] 11%|█▏        | 19/166 [00:36<04:35,  1.88s/it] 12%|█▏        | 20/166 [00:38<04:34,  1.88s/it] 13%|█▎        | 21/166 [00:40<04:32,  1.88s/it] 13%|█▎        | 22/166 [00:42<04:30,  1.88s/it] 14%|█▍        | 23/166 [00:44<04:29,  1.88s/it] 14%|█▍        | 24/166 [00:46<04:27,  1.88s/it] 15%|█▌        | 25/166 [00:47<04:25,  1.88s/it] 16%|█▌        | 26/166 [00:49<04:23,  1.88s/it] 16%|█▋        | 27/166 [00:51<04:22,  1.89s/it] 17%|█▋        | 28/166 [00:53<04:20,  1.89s/it] 17%|█▋        | 29/166 [00:55<04:18,  1.89s/it] 18%|█▊        | 30/166 [00:57<04:16,  1.89s/it] 19%|█▊        | 31/166 [00:59<04:15,  1.89s/it] 19%|█▉        | 32/166 [01:01<04:13,  1.89s/it] 20%|█▉        | 33/166 [01:03<04:11,  1.89s/it] 20%|██        | 34/166 [01:04<04:09,  1.89s/it] 21%|██        | 35/166 [01:06<04:07,  1.89s/it]