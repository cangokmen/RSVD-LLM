/projectnb/manoslab/cgokmen/venvs/rsvd_compress/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/projectnb/manoslab/cgokmen/venvs/rsvd_compress/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/projectnb/manoslab/cgokmen/venvs/rsvd_compress/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/projectnb/manoslab/cgokmen/venvs/rsvd_compress/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.96s/it]
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:31<16:30, 31.95s/it]  6%|▋         | 2/32 [01:04<16:09, 32.30s/it]  9%|▉         | 3/32 [01:37<15:45, 32.61s/it] 12%|█▎        | 4/32 [02:10<15:17, 32.77s/it] 16%|█▌        | 5/32 [02:43<14:47, 32.86s/it] 19%|█▉        | 6/32 [03:16<14:16, 32.93s/it] 22%|██▏       | 7/32 [03:49<13:43, 32.96s/it] 25%|██▌       | 8/32 [04:22<13:11, 32.98s/it] 28%|██▊       | 9/32 [04:55<12:38, 33.00s/it] 31%|███▏      | 10/32 [05:28<12:06, 33.01s/it] 34%|███▍      | 11/32 [06:01<11:33, 33.02s/it] 38%|███▊      | 12/32 [06:34<11:00, 33.03s/it] 41%|████      | 13/32 [07:07<10:27, 33.02s/it] 44%|████▍     | 14/32 [07:40<09:54, 33.03s/it] 47%|████▋     | 15/32 [08:13<09:21, 33.04s/it] 50%|█████     | 16/32 [08:46<08:48, 33.04s/it] 53%|█████▎    | 17/32 [09:19<08:15, 33.04s/it] 56%|█████▋    | 18/32 [09:53<07:42, 33.04s/it] 59%|█████▉    | 19/32 [10:26<07:09, 33.03s/it] 62%|██████▎   | 20/32 [10:59<06:36, 33.03s/it] 66%|██████▌   | 21/32 [11:31<06:02, 32.95s/it] 69%|██████▉   | 22/32 [12:04<05:29, 32.99s/it] 72%|███████▏  | 23/32 [12:37<04:57, 33.02s/it] 75%|███████▌  | 24/32 [13:11<04:24, 33.02s/it] 78%|███████▊  | 25/32 [13:44<03:51, 33.03s/it] 81%|████████▏ | 26/32 [14:17<03:18, 33.04s/it]